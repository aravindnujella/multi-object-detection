{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "import colorsys\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "inf = float('inf')\n",
    "nan = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/aravind/dataset/train2017\"\n",
    "val_dir = \"/home/aravind/dataset/val2017\"\n",
    "train_ann = \"/home/aravind/dataset/annotations/instances_train2017.json\"\n",
    "val_ann = \"/home/aravind/dataset/annotations/instances_val2017.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     17
    ]
   },
   "outputs": [],
   "source": [
    "# config to train\n",
    "# TODO: check Config is correct\n",
    "class ProposalConfig():\n",
    "    NAME = \"InSegm\"\n",
    "    GPU_COUNT = 1\n",
    "    # online training\n",
    "    IMAGES_PER_GPU = 1\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    NUM_WORKERS = 1\n",
    "    PIN_MEMORY = True\n",
    "    VALIDATION_STEPS = 20\n",
    "    # including bg\n",
    "    NUM_CLASSES = 81\n",
    "\n",
    "    MEAN_PIXEL = np.array(\n",
    "        [0.485, 0.456, 0.406], dtype=np.float32).reshape(1, 1, -1)\n",
    "    STD_PIXEL = np.array(\n",
    "        [0.229, 0.224, 0.225], dtype=np.float32).reshape(1, 1, -1)\n",
    "    CLASS_NAMES = [\n",
    "        'BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "        'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
    "        'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "        'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
    "        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "        'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "        'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n",
    "        'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "        'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
    "        'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n",
    "        'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "        'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
    "        'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "    ]\n",
    "    GRID_SHAPE = 11\n",
    "    IMPULSE_SHAPE = (48, 48)\n",
    "    MIN_PIXELS = 256\n",
    "    MIN_INTERSECTION = 128\n",
    "    def __init__(self):\n",
    "        self.WIDTH = 32 * self.GRID_SHAPE\n",
    "        self.HEIGHT = 32 * self.GRID_SHAPE\n",
    "        self.BATCH_SIZE = self.IMAGES_PER_GPU * self.GPU_COUNT\n",
    "        self.IMAGE_SHAPE = (self.WIDTH, self.HEIGHT, 3)\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display Configuration values.\"\"\"\n",
    "        print(\"\\nConfigurations:\")\n",
    "        for a in dir(self):\n",
    "            if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
    "                print(\"{:30} {}\".format(a, getattr(self, a)))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetection(data.Dataset):\n",
    "    def __init__(self, root, annFile, config):\n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        self.config = config\n",
    "        self.catMap = self.build_class_map()\n",
    "\n",
    "    # coco ids remapped to contigous range(81) (including background as 0)\n",
    "    def build_class_map(self):\n",
    "        catMap = {}\n",
    "        coco_cat_ids = [0] + self.coco.getCatIds(config.CLASS_NAMES[1:])\n",
    "        for i in range(81):\n",
    "            catMap[coco_cat_ids[i]] = i\n",
    "        return catMap\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # IO stuff: reading image, masks; decoding masks as numpy arrays\n",
    "        img, instance_masks, class_ids = self.load_data(index)\n",
    "\n",
    "        # Data Augmentation:\n",
    "        # skip for now\n",
    "\n",
    "        # Target generation:\n",
    "        return self.generate_targets(img, instance_masks, class_ids)\n",
    "\n",
    "    # make base impulse\n",
    "    # map each impulse to object with highest overlap\n",
    "\n",
    "    def generate_targets(self, img, instance_masks, class_ids):\n",
    "        config = self.config\n",
    "        # add background mask, background class\n",
    "        # so that all impulses get some response\n",
    "        bg_mask = np.where(np.sum(instance_masks, 0, keepdims=True) == 0, 1, 0)\n",
    "        instance_masks = np.concatenate([bg_mask, instance_masks], 0)\n",
    "        class_ids = np.concatenate([[0], class_ids], 0)\n",
    "        # resize image, masks to 448*448\n",
    "        w, h = config.WIDTH, config.HEIGHT\n",
    "        img = self.resize_image(img, (w, h), \"RGB\")\n",
    "        instance_masks = np.array(\n",
    "            [self.resize_image(m, (w, h), \"L\") for m in instance_masks])\n",
    "        # generate base impulse\n",
    "        base_impulse = self.make_base_impulse()\n",
    "        # map masks, class labels to impulses\n",
    "        # map_freq is number of a single mask is mapped to some impulse\n",
    "        # this is to normalize loss function\n",
    "        mask_response, class_response, freq_normalization = self.map_impulse_response(\n",
    "            base_impulse, instance_masks, class_ids)\n",
    "    \n",
    "        img = (img/255-config.MEAN_PIXEL)/config.STD_PIXEL\n",
    "        img = np.moveaxis(img,2,0)\n",
    "        \n",
    "        data = [img, mask_response, class_response, base_impulse, freq_normalization]\n",
    "        data = [torch.from_numpy(np.array(i).astype(np.float32)) for i in data]\n",
    "        return tuple(data)\n",
    "    # not so fast implementation of iou between two pairs of masks\n",
    "    # a, b binary masks [n,w,h]\n",
    "\n",
    "    def all_pairs_iou(self, a, b):\n",
    "        iou = np.zeros((a.shape[0], b.shape[0]))\n",
    "        for i in range(a.shape[0]):\n",
    "            m = np.expand_dims(a[i], 0)\n",
    "            intersection = np.sum(m * b, (1, 2))\n",
    "            intersection = (intersection >\n",
    "                            self.config.MIN_INTERSECTION) * intersection\n",
    "            union = np.sum(b + m, (1, 2)) - intersection\n",
    "            iou[i, :] = intersection / union\n",
    "        return iou\n",
    "\n",
    "    def map_impulse_response(self, base_impulse, instance_masks, class_ids):\n",
    "        scores = self.all_pairs_iou(base_impulse, instance_masks)\n",
    "        ids = np.argmax(scores, -1)\n",
    "        mask_response = instance_masks[ids]\n",
    "        class_response = class_ids[ids]\n",
    "\n",
    "        freq_normalization = np.ones(base_impulse.shape[0])\n",
    "        counts = np.zeros(instance_masks.shape[0])\n",
    "        for i in range(ids.shape[0]):\n",
    "            counts[ids[i]] += 1\n",
    "        for i in range(ids.shape[0]):\n",
    "            freq_normalization[i] = 1 / counts[ids[i]]\n",
    "\n",
    "        return mask_response, class_response, freq_normalization\n",
    "\n",
    "    # we generate impulses evenly spread across the image\n",
    "    # divide image into grid; this is heuristic\n",
    "    # here, we place impulse centres as from d to (2g-1)*d\n",
    "    # separated by 2*d. where d = w//(2*g)\n",
    "\n",
    "    def make_base_impulse(self):\n",
    "        config = self.config\n",
    "        g = config.GRID_SHAPE\n",
    "        w, h = config.WIDTH, config.HEIGHT\n",
    "        dw, dh = w // (2 * g), h // (2 * g)\n",
    "\n",
    "        base_impulse = np.zeros((g * g, w, h))\n",
    "        dx, dy = config.IMPULSE_SHAPE[0] // 2, config.IMPULSE_SHAPE[1] // 2\n",
    "\n",
    "        for i in range(g):\n",
    "            for j in range(g):\n",
    "                k = g * i + j\n",
    "                x, y = dw * (2 * i + 1), dh * (2 * j + 1)\n",
    "                lx = max(0, x - dx)\n",
    "                ly = max(0, y - dy)\n",
    "                rx = min(x + dx, w)\n",
    "                ry = min(y + dy, h)\n",
    "                base_impulse[k][lx:rx, ly:ry] = np.ones((rx - lx, ry - ly))\n",
    "        return base_impulse\n",
    "\n",
    "    # resize image/mask to specified size without losing aspect ratio\n",
    "\n",
    "    def resize_image(self, img, size, mode):\n",
    "        interpolation = {\"RGB\": Image.BICUBIC, \"L\": Image.NEAREST}[mode]\n",
    "        img_obj = Image.fromarray(img.astype(np.uint8), mode)\n",
    "        img_obj.thumbnail(size, interpolation)\n",
    "\n",
    "        (w, h) = img_obj.size\n",
    "        padded_img = Image.new(mode, size, \"black\")\n",
    "        padded_img.paste(img_obj, ((size[0] - w) // 2, (size[1] - h) // 2))\n",
    "\n",
    "        return np.array(padded_img)\n",
    "\n",
    "    # read image, masks; decode masks to numpy arrays\n",
    "    # image format: channels last\n",
    "    # mask format: channels first\n",
    "\n",
    "    def load_data(self, index):\n",
    "        coco = self.coco\n",
    "        config = self.config\n",
    "\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(\n",
    "            imgIds=img_id, areaRng=[config.MIN_PIXELS, inf], iscrowd=False)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        w, h = img.size\n",
    "        for ann in anns:\n",
    "            instance_masks.append(self.annToMask(ann, h, w))\n",
    "            class_ids.append(self.catMap[ann['category_id']])\n",
    "        return np.array(img), np.array(instance_masks), np.array(class_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(\n",
    "            tmp,\n",
    "            self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(\n",
    "            tmp,\n",
    "            self.target_transform.__repr__().replace('\\n',\n",
    "                                                     '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "    def annToRLE(self, ann, h, w):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "\n",
    "        segm = ann['segmentation']\n",
    "        if type(segm) == list:\n",
    "            # polygon -- a single object might consist of multiple parts\n",
    "            # we merge all parts into one mask rle code\n",
    "            rles = maskUtils.frPyObjects(segm, h, w)\n",
    "            rle = maskUtils.merge(rles)\n",
    "        elif type(segm['counts']) == list:\n",
    "            # uncompressed RLE\n",
    "            rle = maskUtils.frPyObjects(segm, h, w)\n",
    "        else:\n",
    "            # rle\n",
    "            rle = ann['segmentation']\n",
    "        return rle\n",
    "\n",
    "    def annToMask(self, ann, h, w):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        rle = self.annToRLE(ann, h, w)\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.59s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "config = ProposalConfig()\n",
    "val_dataset = CocoDetection(val_dir, val_ann, config)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=config.BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=config.PIN_MEMORY,\n",
    "                                          num_workers=config.NUM_WORKERS\n",
    "                                          )\n",
    "# train_dataset = CocoDetection(train_dir,train_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = next(q)[0]\n",
    "# print(imgs.shape)\n",
    "# Image.fromarray(np.array(imgs[0]).astype(np.uint8),\"RGB\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (shitty make shift) modification of vgg 16.\n",
    "# exotic variations postponed\n",
    "\n",
    "\n",
    "class split_conv(nn.Module):\n",
    "    def __init__(self, in_features, cur_out, d_out):\n",
    "        super(split_conv, self).__init__()\n",
    "        self.ignore_filters = nn.Conv2d(\n",
    "            in_features, cur_out, (3, 3), padding=(1, 1))\n",
    "        self.copy_filters = nn.Conv2d(\n",
    "            in_features, d_out, (3, 3), padding=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        ignore = self.ignore_filters(x)\n",
    "        copy = self.copy_filters(x)\n",
    "        return torch.cat([ignore, copy], 1)\n",
    "\n",
    "\n",
    "class split_vgg16_features(nn.Module):\n",
    "    def __init__(self, pre_trained_weights, d_in):\n",
    "        super(split_vgg16_features, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            split_conv(3 + d_in, 64, d_in),\n",
    "            self.relu,\n",
    "            split_conv(64 + d_in, 64, d_in),\n",
    "            self.relu,\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            split_conv(64 + d_in, 128, d_in),\n",
    "            self.relu,\n",
    "            split_conv(128 + d_in, 128, d_in),\n",
    "            self.relu,\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            split_conv(128 + d_in, 256, d_in),\n",
    "            self.relu,\n",
    "            split_conv(256 + d_in, 256, d_in),\n",
    "            self.relu,\n",
    "            split_conv(256 + d_in, 256, d_in),\n",
    "            self.relu,\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            split_conv(256 + d_in, 512, d_in),\n",
    "            self.relu,\n",
    "            split_conv(512 + d_in, 512, d_in),\n",
    "            self.relu,\n",
    "            split_conv(512 + d_in, 512, d_in),\n",
    "            self.relu,\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            split_conv(512 + d_in, 512, d_in),\n",
    "            self.relu,\n",
    "            split_conv(512 + d_in, 512, d_in),\n",
    "            self.relu,\n",
    "            split_conv(512 + d_in, 512, d_in),\n",
    "            self.relu,\n",
    "        )\n",
    "\n",
    "        # initialize with vgg weights\n",
    "        if pre_trained_weights == True:\n",
    "            self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        x = self.layer1(x)\n",
    "        outs.append(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        outs.append(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        outs.append(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        outs.append(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.layer5(x)\n",
    "        outs.append(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return x, outs\n",
    "\n",
    "    def init_weights(self):\n",
    "        _shapes = [[] for i in range(5)]\n",
    "        l = 0\n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        for child in vgg.features.children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                _shapes[l].append(child.weight.shape)\n",
    "            elif isinstance(child, nn.MaxPool2d):\n",
    "                l += 1\n",
    "\n",
    "        d_in = self.d_in\n",
    "\n",
    "        copy_weight = [[] for l in range(5)]\n",
    "        ignore_weight = [[] for l in range(5)]\n",
    "        ignore_bias = [[] for l in range(5)]\n",
    "        copy_bias = [[] for l in range(5)]\n",
    "\n",
    "        i = 0\n",
    "        l = 0\n",
    "        decay = 1.1\n",
    "\n",
    "        for child in vgg.features.children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                cur_in = _shapes[l][i][1]\n",
    "                cur_out = _shapes[l][i][0]\n",
    "                kernel_shape = _shapes[l][i][2:]\n",
    "                # !!!\n",
    "                d_out = d_in\n",
    "                fan_in = kernel_shape[0] * kernel_shape[1]\n",
    "                # ignore_filters: cur_out, cur_in + d_in, kernel_shape\n",
    "                c = torch.zeros((cur_out, d_in) + kernel_shape)\n",
    "                ignore_filters = torch.cat([child.weight, c], 1)\n",
    "                # copy_filters: d_out, cur_in + d_in, kernel_shape\n",
    "                a = torch.zeros((\n",
    "                    d_out,\n",
    "                    cur_in,\n",
    "                ) + kernel_shape)\n",
    "                b = torch.eye(d_out, d_in).unsqueeze(-1).unsqueeze(-1).float()\n",
    "                b = b.repeat([1, 1, kernel_shape[0], kernel_shape[1]\n",
    "                              ]) / fan_in / random.uniform(1, decay)\n",
    "                copy_filters = torch.cat([a, b], 1)\n",
    "                ignore_weight[l].append(ignore_filters)\n",
    "                ignore_bias[l].append(child.bias)\n",
    "                copy_weight[l].append(copy_filters)\n",
    "                copy_bias[l].append(torch.zeros(d_out))\n",
    "                d_in = d_out\n",
    "                i += 1\n",
    "            elif isinstance(child, nn.MaxPool2d):\n",
    "                l += 1\n",
    "                i = 0\n",
    "\n",
    "        l = 0\n",
    "        for name, child in self.named_children():\n",
    "            if name[:-1] == \"layer\":\n",
    "                k = 0\n",
    "                for gc in child.children():\n",
    "                    if isinstance(gc, split_conv):\n",
    "                        gc.copy_filters.weight = nn.Parameter(\n",
    "                            copy_weight[l][k])\n",
    "                        gc.ignore_filters.weight = nn.Parameter(\n",
    "                            ignore_weight[l][k])\n",
    "                        gc.copy_filters.bias = nn.Parameter(copy_bias[l][k])\n",
    "                        gc.ignore_filters.bias = nn.Parameter(\n",
    "                            ignore_bias[l][k])\n",
    "                        k += 1\n",
    "                l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskProp(nn.Module):\n",
    "    def __init__(self, init_weights, d_in):\n",
    "        super(MaskProp, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                512 + d_in + 512 + d_in, 512 + d_in, (3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(512 + d_in),\n",
    "            self.relu,\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                512 + d_in + 512 + d_in, 512 + d_in, (3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(512 + d_in),\n",
    "            self.relu,\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                512 + d_in + 256 + d_in, 256 + d_in, (3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(256 + d_in),\n",
    "            self.relu,\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                256 + d_in + 128 + d_in, 128 + d_in, (3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(128 + d_in),\n",
    "            self.relu,\n",
    "        )\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(128 + d_in + 64 + d_in, d_in, (3, 3), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(d_in),\n",
    "            # self.relu,\n",
    "        )\n",
    "\n",
    "        if init_weights:\n",
    "            for name, child in self.named_children():\n",
    "                if name[:-1] == 'layer' or 'mask_layer':\n",
    "                    for gc in child.children():\n",
    "                        if isinstance(gc, nn.Conv2d):\n",
    "                            nn.init.xavier_uniform_(gc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c, m = x\n",
    "        l1, l2, l3, l4, l5 = m\n",
    "\n",
    "        c = F.upsample(c, scale_factor=2)\n",
    "        y = self.layer5(torch.cat([c, l5], 1))\n",
    "\n",
    "        y = self.upsample(y)\n",
    "        y = self.layer4(torch.cat([y, l4], 1))\n",
    "\n",
    "        y = self.upsample(y)\n",
    "        y = self.layer3(torch.cat([y, l3], 1))\n",
    "\n",
    "        y = self.upsample(y)\n",
    "        y = self.layer2(torch.cat([y, l2], 1))\n",
    "\n",
    "        y = self.upsample(y)\n",
    "        y = self.layer1(torch.cat([y, l1], 1))\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "# classifier takes a single level features and classifies\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, init_weights,d_in):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(512+d_in, 512, (3, 3), padding=(1, 1))\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        if init_weights:\n",
    "            nn.init.xavier_uniform_(self.conv1.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SingleHGModel(nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super(SingleHGModel, self).__init__()\n",
    "        self.vgg0 = split_vgg16_features(pre_trained_weights=True, d_in=d_in)\n",
    "        self.mp0 = MaskProp(init_weights=True,d_in = d_in)\n",
    "        self.class_predictor = Classifier(init_weights=True, d_in=d_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        im, impulse = x\n",
    "\n",
    "        inp = torch.cat([im, impulse], dim=1)\n",
    "        class_features, mask_features = self.vgg0(inp)\n",
    "        c = self.class_predictor(class_features)\n",
    "        m0 = self.mp0([class_features, mask_features])\n",
    "\n",
    "        return c, m0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SingleHGModel(d_in=121)\n",
    "net = net.cuda()\n",
    "img, masks, class_ids, base_impulse, freq_normalization = [v.cuda() for v in next(iter(val_loader))]\n",
    "res = net([img,base_impulse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
